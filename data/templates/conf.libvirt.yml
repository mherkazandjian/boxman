---
version: 1.0
project: minimal
provider:
  libvirt:
    uri: qemu:///system
    use_sudo: True
    virt_install_cmd: '/bin/python3 /usr/bin/virt-install'
    virt_clone_cmd: '/bin/python3 /usr/bin/virt-clone'
    virsh_cmd: '/bin/virsh'
  verbose: True

clusters:
  cluster_1:
    # workdir: ~/workspaces/boxmandev/minimal/cluster_1  # auto-derived from workspace.path / cluster_name
    base_image: rocky-95-minimal-base-template
    proxy_host: localhost
    admin_user: 'root'
    admin_pass: '${env:BOXMAN_ADMIN_PASS}'
    # admin_pass: 'file:///abs/or/relative/path'
    # admin_pass: 'file://~/path/to/file'
    admin_key_name: id_ed25519_boxman    # generated automatically at provisioning time
    ssh_config: ssh_config               # generated automatically at provisioning time

    networks:
      nat1:
        mode: nat
        bridge:
          stp: 'on'
          delay: '0'
        mac: '52:54:00:00:00:01'
        ip:
          address: '192.168.123.1'
          netmask: '255.255.255.0'
          dhcp:
            range:
              start: '192.168.123.2'
              end: '192.168.123.254'
        enable: True
        autostart: True
      isolated1:
        mode: route
        bridge:
          stp: 'on'
          delay: '0'
        mac: '52:54:00:00:00:02'
        ip:
          address: '10.0.10.1'
          netmask: '255.255.255.0'
          dhcp:                   # (optional) enable DHCP server on this network
            range:                #   if set this full block needs to be specified
              start: '10.0.10.2'  #   mandatory (if block is specified)
              end: '10.0.10.254'  #   mandatory (if block is specified)
        enable: True
        autostart: True

    vms:
      boxman01:
        hostname: boxman01
        disks:
          - name: disk01
            driver:
              name: qemu
              type: qcow2
            target: vdb
            size: 2048
        cpus:
          sockets: 1
          cores: 1
          threads: 1
        memory: 2048
        network_adapters:
          - name: adapter_1
            link_state: 'up'
            network_source: 'nat1'
            #network_source: 'cluster_1::nat1'
            #network_source: 'myprj::cluster_1_nat1'
          - name: adapter_2
            link_state: 'up'
            network_source: 'isolated1'
      boxman02:
        hostname: boxman02
        disks:
          {% for suffix in 'bcde' %}        # add disks vdb, vdc, vdd, vde
          - name: disk{{ suffix }}
            driver:
              name: qemu
              type: qcow2
            target: vd{{ suffix }}
            size: 2048
          {% endfor %}
        network_adapters:
          - name: adapter_1
            link_state: 'up'
            network_source: 'nat1'
          - name: adapter_2
            link_state: 'up'
            network_source: 'isolated1'
          - name: adapter_3
            link_state: 'up'
            network_source: 'default'
            is_global: True             # set to True to use the global network without expanding the network name to the project scope
      {% for i in range(3, 8) %}
      boxman{{ "%02d" % i }}:
        hostname: boxman{{ "%02d" % i }}
        disks:
          - name: disk01
            driver:
              name: qemu
              type: qcow2
            target: vdb
            size: 2048
        network_adapters:
          - name: adapter_1
            link_state: 'up'
            network_source: 'nat1'
          - name: adapter_2
            link_state: 'up'
            network_source: 'isolated1'
      {% endfor %}
    # files:                                  # (optional) override auto-generated files
    #   env.sh: |                              #   custom env.sh overrides the default
    #     export INVENTORY=inventory
    #     export SALTMASTER=saltmaster01
    #     ...

# workspace.path is the base directory for all cluster workdirs.
# each cluster gets a subdirectory: workspace.path / cluster_name
# e.g. ~/workspaces/boxmandev/minimal/cluster_1
#
# if a cluster defines an explicit 'workdir', that takes precedence.
#
# auto-generated in workspace.path (unless explicitly defined in workspace.files):
#   - env.sh                  (INVENTORY, SSH_CONFIG, GATEWAYHOST, ANSIBLE_CONFIG, etc.)
#   - inventory/01-hosts.yml  (from the VMs defined across all clusters)
#   - ansible.cfg
#   - ssh_config              (generated at provision time by boxman)
#   - ssh keys                (generated at provision time by boxman)
workspace:
  path: ~/workspaces/boxmandev/minimal

# tasks for 'boxman run <task>'
# commands have access to env vars from the workspace env_file
tasks:
  ping:
    description: "ping all hosts via ansible"
    command: >
      ansible -i ${INVENTORY}
      --ssh-extra-args '-F ${SSH_CONFIG}'
      all -m ansible.builtin.ping

  site:
    description: "run ansible site playbook"
    command: >
      ansible-playbook -i ${INVENTORY}
      --ssh-extra-args '-F ${SSH_CONFIG}'
      --become ansible/site.yml

  cmd:
    description: "run a shell command on all hosts"
    command: >
      ansible -i ${INVENTORY}
      --ssh-extra-args '-F ${SSH_CONFIG}'
      all -m ansible.builtin.shell -a

  ssh:
    description: "ssh to the gateway host"
    command: ssh -F ${SSH_CONFIG} -t ${GATEWAYHOST}

  salt-ping:
    description: "salt test.ping on all minions"
    command: >
      ssh -F ${SSH_CONFIG} ${SALTMASTER}
      "sudo salt '*' test.ping -t 120"

  salt-apply:
    description: "apply salt states on all minions"
    command: >
      ssh -F ${SSH_CONFIG} ${SALTMASTER}
      "sudo salt '*' state.apply -t 120"

  salt-sync:
    description: "full salt sync (sync_all, states, pillars)"
    command: >
      ssh -F ${SSH_CONFIG} ${SALTMASTER} "sudo salt '*' saltutil.sync_all -t 120" &&
      ssh -F ${SSH_CONFIG} ${SALTMASTER} "sudo salt '*' saltutil.sync_states -t 120" &&
      ssh -F ${SSH_CONFIG} ${SALTMASTER} "sudo salt '*' saltutil.refresh_pillar -t 120" &&
      ssh -F ${SSH_CONFIG} ${SALTMASTER} "sudo salt '*' saltutil.sync_all -t 120"
